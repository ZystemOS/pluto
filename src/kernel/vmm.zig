const build_options = @import("build_options");
const mock_path = build_options.mock_path;
const builtin = std.builtin;
const is_test = builtin.is_test;
const std = @import("std");
const log = std.log.scoped(.vmm);
const bitmap = @import("bitmap.zig");
const pmm = @import("pmm.zig");
const mem = @import("mem.zig");
const tty = @import("tty.zig");
const panic = @import("panic.zig").panic;
const arch = @import("arch.zig").internals;
const Allocator = std.mem.Allocator;
const assert = std.debug.assert;

/// Attributes for a virtual memory allocation
pub const Attributes = struct {
    /// Whether this memory belongs to the kernel and can therefore not be accessed in user mode
    kernel: bool,

    /// If this memory can be written to
    writable: bool,

    /// If this memory can be cached. Memory mapped to a device shouldn't, for example
    cachable: bool,
};

/// All data that must be remembered for a virtual memory allocation
const Allocation = struct {
    /// The physical blocks of memory associated with this allocation
    physical: std.ArrayList(usize),
};

/// The size of each allocatable block, the same as the physical memory manager's block size
pub const BLOCK_SIZE: usize = pmm.BLOCK_SIZE;

pub const MapperError = error{
    InvalidVirtualAddress,
    InvalidPhysicalAddress,
    AddressMismatch,
    MisalignedVirtualAddress,
    MisalignedPhysicalAddress,
    NotMapped,
};

///
/// Returns a container that can map and unmap virtual memory to physical memory.
/// The mapper can pass some payload data when mapping an unmapping, which is of type `Payload`. This can be anything that the underlying mapper needs to carry out the mapping process.
/// For x86, it would be the page directory that is being mapped within. An architecture or other mapper can specify the data it needs when mapping by specifying this type.
///
/// Arguments:
///     IN comptime Payload: type - The type of the VMM-specific payload to pass when mapping and unmapping
///
/// Return: type
///     The Mapper type constructed.
///
pub fn Mapper(comptime Payload: type) type {
    return struct {
        ///
        /// Map a region (can span more than one block) of virtual memory to physical memory. After a call to this function, the memory should be present the next time it is accessed.
        /// The attributes given must be obeyed when possible.
        ///
        /// Arguments:
        ///     IN virtual_start: usize - The start of the virtual memory to map
        ///     IN virtual_end: usize - The end of the virtual memory to map
        ///     IN physical_start: usize - The start of the physical memory to map to
        ///     IN physical_end: usize - The end of the physical memory to map to
        ///     IN attrs: Attributes - The attributes to apply to this region of memory
        ///     IN/OUT allocator: Allocator - The allocator to use when mapping, if required
        ///     IN spec: Payload - The payload to pass to the mapper
        ///
        /// Error: AllocatorError || MapperError
        ///     The causes depend on the mapper used
        ///
        mapFn: fn (virtual_start: usize, virtual_end: usize, physical_start: usize, physical_end: usize, attrs: Attributes, allocator: Allocator, spec: Payload) (Allocator.Error || MapperError)!void,

        ///
        /// Unmap a region (can span more than one block) of virtual memory from its physical memory. After a call to this function, the memory should not be accessible without error.
        ///
        /// Arguments:
        ///     IN virtual_start: usize - The start of the virtual region to unmap
        ///     IN virtual_end: usize - The end of the virtual region to unmap
        ///     IN/OUT allocator: Allocator - The allocator to use to free the mapping
        ///     IN spec: Payload - The payload to pass to the mapper
        ///
        /// Error: MapperError
        ///     The causes depend on the mapper used
        ///
        unmapFn: fn (virtual_start: usize, virtual_end: usize, allocator: Allocator, spec: Payload) MapperError!void,
    };
}

/// Errors that can be returned by VMM functions
pub const VmmError = error{
    /// A memory region expected to be allocated wasn't
    NotAllocated,

    /// A memory region expected to not be allocated was
    AlreadyAllocated,

    /// A physical memory region expected to not be allocated was
    PhysicalAlreadyAllocated,

    /// A physical region of memory isn't of the same size as a virtual region
    PhysicalVirtualMismatch,

    /// Virtual addresses are invalid
    InvalidVirtAddresses,

    /// Physical addresses are invalid
    InvalidPhysAddresses,

    /// Not enough virtual space in the VMM
    OutOfMemory,
};

/// The boot-time offset that the virtual addresses are from the physical addresses
/// This is the start of the memory owned by the kernel and so is where the kernel VMM starts
extern var KERNEL_ADDR_OFFSET: *u32;

/// The virtual memory manager associated with the kernel address space
pub var kernel_vmm: VirtualMemoryManager(arch.VmmPayload) = undefined;

///
/// Construct a virtual memory manager to keep track of allocated and free virtual memory regions within a certain space
///
/// Arguments:
///     IN comptime Payload: type - The type of the payload to pass to the mapper
///
/// Return: type
///     The constructed type
///
pub fn VirtualMemoryManager(comptime Payload: type) type {
    return struct {
        /// The bitmap that keeps track of allocated and free regions
        bmp: bitmap.Bitmap(false, usize, null),

        /// The start of the memory to be tracked
        start: usize,

        /// The end of the memory to be tracked
        end: usize,

        /// The allocator to use when allocating and freeing regions
        allocator: Allocator,

        /// All allocations that have been made with this manager
        allocations: std.hash_map.AutoHashMap(usize, Allocation),

        /// The mapper to use when allocating and freeing regions
        mapper: Mapper(Payload),

        /// The payload to pass to the mapper functions
        payload: Payload,

        const Self = @This();

        ///
        /// Initialise a virtual memory manager
        ///
        /// Arguments:
        ///     IN start: usize - The start of the memory region to manage
        ///     IN end: usize - The end of the memory region to manage. Must be greater than the start
        ///     IN/OUT allocator: Allocator - The allocator to use when allocating and freeing regions
        ///     IN mapper: Mapper - The mapper to use when allocating and freeing regions
        ///     IN payload: Payload - The payload data to be passed to the mapper
        ///
        /// Return: Self
        ///     The manager constructed
        ///
        /// Error: Allocator.Error
        ///     error.OutOfMemory - The allocator cannot allocate the memory required
        ///
        pub fn init(start: usize, end: usize, allocator: Allocator, mapper: Mapper(Payload), payload: Payload) Allocator.Error!Self {
            const size = end - start;
            var bmp = try bitmap.Bitmap(false, usize, null).init(std.mem.alignForward(size, pmm.BLOCK_SIZE) / pmm.BLOCK_SIZE, allocator);
            return Self{
                .bmp = bmp,
                .start = start,
                .end = end,
                .allocator = allocator,
                .allocations = std.hash_map.AutoHashMap(usize, Allocation).init(allocator),
                .mapper = mapper,
                .payload = payload,
            };
        }

        ///
        /// Copy this VMM. Changes to one copy will not affect the other
        ///
        /// Arguments:
        ///     IN self: *Self - The VMM to copy
        ///
        /// Error: Allocator.Error
        ///     OutOfMemory - There wasn't enough memory for copying
        ///
        /// Return: Self
        ///     The copy
        ///
        pub fn copy(self: *const Self) Allocator.Error!Self {
            var clone = Self{
                .bmp = try self.bmp.clone(),
                .start = self.start,
                .end = self.end,
                .allocator = self.allocator,
                .allocations = std.hash_map.AutoHashMap(usize, Allocation).init(self.allocator),
                .mapper = self.mapper,
                .payload = self.payload,
            };
            var it = self.allocations.iterator();
            while (it.next()) |entry| {
                var list = std.ArrayList(usize).init(self.allocator);
                for (entry.value_ptr.physical.items) |block| {
                    _ = try list.append(block);
                }
                _ = try clone.allocations.put(entry.key_ptr.*, Allocation{ .physical = list });
            }
            return clone;
        }

        ///
        /// Free the internal state of the VMM. It is unusable afterwards
        ///
        /// Arguments:
        ///     IN self: *Self - The VMM to deinitialise
        ///
        pub fn deinit(self: *Self) void {
            self.bmp.deinit();
            var it = self.allocations.iterator();
            while (it.next()) |entry| {
                entry.value_ptr.physical.deinit();
            }
            self.allocations.deinit();
        }

        ///
        /// Find the physical address that a given virtual address is mapped to.
        ///
        /// Arguments:
        ///     IN self: *const Self - The VMM to check for mappings in
        ///     IN virt: usize - The virtual address to find the physical address for
        ///
        /// Return: usize
        ///     The physical address that the virtual address is mapped to
        ///
        /// Error: VmmError
        ///     VmmError.NotAllocated - The virtual address hasn't been mapped within the VMM
        ///
        pub fn virtToPhys(self: *const Self, virt: usize) VmmError!usize {
            var it = self.allocations.iterator();
            while (it.next()) |entry| {
                const vaddr = entry.key_ptr.*;

                const allocation = entry.value_ptr.*;
                // If this allocation range covers the virtual address then figure out the corresponding physical block
                if (vaddr <= virt and vaddr + (allocation.physical.items.len * BLOCK_SIZE) > virt) {
                    const block_number = (virt - vaddr) / BLOCK_SIZE;
                    const block_offset = (virt - vaddr) % BLOCK_SIZE;
                    return allocation.physical.items[block_number] + block_offset;
                }
            }
            return VmmError.NotAllocated;
        }

        ///
        /// Find the virtual address that a given physical address is mapped to.
        ///
        /// Arguments:
        ///     IN self: *const Self - The VMM to check for mappings in
        ///     IN phys: usize - The physical address to find the virtual address for
        ///
        /// Return: usize
        ///     The virtual address that the physical address is mapped to
        ///
        /// Error: VmmError
        ///     VmmError.NotAllocated - The physical address hasn't been mapped within the VMM
        ///
        pub fn physToVirt(self: *const Self, phys: usize) VmmError!usize {
            var it = self.allocations.iterator();
            while (it.next()) |entry| {
                const vaddr = entry.key_ptr.*;
                const allocation = entry.value_ptr.*;

                for (allocation.physical.items) |block, i| {
                    if (block <= phys and block + BLOCK_SIZE > phys) {
                        const block_addr = vaddr + i * BLOCK_SIZE;
                        const block_offset = phys % BLOCK_SIZE;
                        return block_addr + block_offset;
                    }
                }
            }
            return VmmError.NotAllocated;
        }

        ///
        /// Check if a virtual memory address has been set
        ///
        /// Arguments:
        ///     IN self: *Self - The manager to check
        ///     IN virt: usize - The virtual memory address to check
        ///
        /// Return: bool
        ///     Whether the address is set
        ///
        /// Error: pmm.PmmError
        ///     Bitmap(u32).Error.OutOfBounds - The address given is outside of the memory managed
        ///
        pub fn isSet(self: *const Self, virt: usize) bitmap.BitmapError!bool {
            return self.bmp.isSet((virt - self.start) / BLOCK_SIZE);
        }

        ///
        /// Map a region (can span more than one block) of virtual memory to a specific region of memory
        ///
        /// Arguments:
        ///     IN/OUT self: *Self - The manager to modify
        ///     IN virtual: mem.Range - The virtual region to set
        ///     IN physical: ?mem.Range - The physical region to map to or null if only the virtual region is to be set
        ///     IN attrs: Attributes - The attributes to apply to the memory regions
        ///
        /// Error: VmmError || Bitmap(u32).BitmapError || Allocator.Error || MapperError
        ///     VmmError.AlreadyAllocated - The virtual address has already been allocated
        ///     VmmError.PhysicalAlreadyAllocated - The physical address has already been allocated
        ///     VmmError.PhysicalVirtualMismatch - The physical region and virtual region are of different sizes
        ///     VmmError.InvalidVirtAddresses - The start virtual address is greater than the end address
        ///     VmmError.InvalidPhysicalAddresses - The start physical address is greater than the end address
        ///     Bitmap.BitmapError.OutOfBounds - The physical or virtual addresses are out of bounds
        ///     Allocator.Error.OutOfMemory - Allocating the required memory failed
        ///     MapperError.* - The causes depend on the mapper used
        ///
        pub fn set(self: *Self, virtual: mem.Range, physical: ?mem.Range, attrs: Attributes) (VmmError || bitmap.BitmapError || Allocator.Error || MapperError)!void {
            var virt = virtual.start;
            while (virt < virtual.end) : (virt += BLOCK_SIZE) {
                if (try self.isSet(virt)) {
                    return VmmError.AlreadyAllocated;
                }
            }
            if (virtual.start > virtual.end) {
                return VmmError.InvalidVirtAddresses;
            }

            if (physical) |p| {
                if (virtual.end - virtual.start != p.end - p.start) {
                    return VmmError.PhysicalVirtualMismatch;
                }
                if (p.start > p.end) {
                    return VmmError.InvalidPhysAddresses;
                }
                var phys = p.start;
                while (phys < p.end) : (phys += BLOCK_SIZE) {
                    if (try pmm.isSet(phys)) {
                        return VmmError.PhysicalAlreadyAllocated;
                    }
                }
            }

            var phys_list = std.ArrayList(usize).init(self.allocator);

            virt = virtual.start;
            while (virt < virtual.end) : (virt += BLOCK_SIZE) {
                try self.bmp.setEntry((virt - self.start) / BLOCK_SIZE);
            }

            if (physical) |p| {
                var phys = p.start;
                while (phys < p.end) : (phys += BLOCK_SIZE) {
                    try pmm.setAddr(phys);
                    try phys_list.append(phys);
                }
            }

            // Do this before mapping as the mapper may depend on the allocation being tracked
            _ = try self.allocations.put(virtual.start, Allocation{ .physical = phys_list });

            if (physical) |p| {
                try self.mapper.mapFn(virtual.start, virtual.end, p.start, p.end, attrs, self.allocator, self.payload);
            }
        }

        ///
        /// Allocate a number of contiguous blocks of virtual memory
        ///
        /// Arguments:
        ///     IN/OUT self: *Self - The manager to allocate for
        ///     IN num: usize - The number of blocks to allocate
        ///     IN virtual_addr: ?usize - The virtual address to allocate to or null if any address is acceptable
        ///     IN attrs: Attributes - The attributes to apply to the mapped memory
        ///
        /// Return: ?usize
        ///     The address at the start of the allocated region, or null if no region could be allocated due to a lack of contiguous blocks.
        ///
        /// Error: Allocator.Error
        ///     error.OutOfMemory: The required amount of memory couldn't be allocated
        ///
        pub fn alloc(self: *Self, num: usize, virtual_addr: ?usize, attrs: Attributes) Allocator.Error!?usize {
            if (num == 0) {
                return null;
            }
            // Ensure that there is both enough physical and virtual address space free
            if (pmm.blocksFree() >= num and self.bmp.num_free_entries >= num) {
                // The virtual address space must be contiguous
                // Allocate from a specific entry if the caller requested it
                if (self.bmp.setContiguous(num, if (virtual_addr) |a| (a - self.start) / BLOCK_SIZE else null)) |entry| {
                    var block_list = std.ArrayList(usize).init(self.allocator);
                    try block_list.ensureUnusedCapacity(num);

                    var i: usize = 0;
                    const vaddr_start = self.start + entry * BLOCK_SIZE;
                    var vaddr = vaddr_start;
                    // Map the blocks to physical memory
                    while (i < num) : (i += 1) {
                        const addr = pmm.alloc() orelse unreachable;
                        try block_list.append(addr);
                        // The map function failing isn't the caller's responsibility so panic as it shouldn't happen
                        self.mapper.mapFn(vaddr, vaddr + BLOCK_SIZE, addr, addr + BLOCK_SIZE, attrs, self.allocator, self.payload) catch |e| {
                            panic(@errorReturnTrace(), "Failed to map virtual memory: {X}\n", .{e});
                        };
                        vaddr += BLOCK_SIZE;
                    }
                    _ = try self.allocations.put(vaddr_start, Allocation{ .physical = block_list });
                    return vaddr_start;
                }
            }
            return null;
        }

        ///
        /// Copy data from an address in a virtual memory manager to an address in another virtual memory manager
        ///
        /// Arguments:
        ///     IN self: *Self - One of the VMMs to copy between. This should be the currently active VMM
        ///     IN other: *Self - The second of the VMMs to copy between
        ///     IN from: bool - Whether the data should be copied from `self` to `other`, or the other way around
        ///     IN data: if (from) []const u8 else []u8 - The being copied from or written to (depending on `from`). Must be mapped within the VMM being copied from/to
        ///     IN address: usize - The address within `other` that is to be copied from or to
        ///
        /// Error: VmmError || pmm.PmmError || Allocator.Error
        ///     VmmError.NotAllocated - Some or all of the destination isn't mapped
        ///     VmmError.OutOfMemory - There wasn't enough space in the VMM to use for temporary mapping
        ///     Bitmap(u32).Error.OutOfBounds - The address given is outside of the memory managed
        ///     Allocator.Error.OutOfMemory - There wasn't enough memory available to fulfill the request
        ///
        pub fn copyData(self: *Self, other: *const Self, comptime from: bool, data: if (from) []const u8 else []u8, address: usize) (bitmap.BitmapError || VmmError || Allocator.Error)!void {
            if (data.len == 0) {
                return;
            }
            const start_addr = std.mem.alignBackward(address, BLOCK_SIZE);
            const end_addr = std.mem.alignForward(address + data.len, BLOCK_SIZE);
            if (end_addr >= other.end or start_addr < other.start)
                return bitmap.BitmapError.OutOfBounds;
            // Find physical blocks for the address
            var blocks = std.ArrayList(usize).init(self.allocator);
            defer blocks.deinit();
            var it = other.allocations.iterator();
            while (it.next()) |allocation| {
                const virtual = allocation.key_ptr.*;
                const physical = allocation.value_ptr.*.physical.items;
                if (start_addr >= virtual and virtual + physical.len * BLOCK_SIZE >= end_addr) {
                    const first_block_idx = (start_addr - virtual) / BLOCK_SIZE;
                    const last_block_idx = (end_addr - virtual) / BLOCK_SIZE;

                    try blocks.appendSlice(physical[first_block_idx..last_block_idx]);
                }
            }
            // Make sure the address is actually mapped in the destination VMM
            if (blocks.items.len != std.mem.alignForward(data.len, BLOCK_SIZE) / BLOCK_SIZE) {
                return VmmError.NotAllocated;
            }

            // Map them into self for some vaddr so they can be accessed from this VMM
            if (self.bmp.setContiguous(blocks.items.len, null)) |entry| {
                const v_start = entry * BLOCK_SIZE + self.start;
                for (blocks.items) |block, i| {
                    const v = v_start + i * BLOCK_SIZE;
                    const v_end = v + BLOCK_SIZE;
                    const p = block;
                    const p_end = p + BLOCK_SIZE;
                    self.mapper.mapFn(v, v_end, p, p_end, .{ .kernel = true, .writable = true, .cachable = false }, self.allocator, self.payload) catch |e| {
                        // If we fail to map one of the blocks then attempt to free all previously mapped
                        if (i > 0) {
                            self.mapper.unmapFn(v_start, v_end, self.allocator, self.payload) catch |e2| {
                                // If we can't unmap then just panic
                                panic(@errorReturnTrace(), "Failed to unmap virtual region 0x{X} -> 0x{X}: {}\n", .{ v_start, v_end, e2 });
                            };
                        }
                        panic(@errorReturnTrace(), "Failed to map virtual region 0x{X} -> 0x{X} to 0x{X} -> 0x{X}: {}\n", .{ v, v_end, p, p_end, e });
                    };
                }
                // Copy to vaddr from above
                const align_offset = address - start_addr;
                var data_copy = @intToPtr([*]u8, v_start + align_offset)[0..data.len];
                if (from) {
                    std.mem.copy(u8, data_copy, data);
                } else {
                    std.mem.copy(u8, data, data_copy);
                }
                // TODO Unmap and freee virtual blocks from self so they can be used in the future
            } else {
                return VmmError.OutOfMemory;
            }
        }

        ///
        /// Free a previous allocation
        ///
        /// Arguments:
        ///     IN/OUT self: *Self - The manager to free within
        ///     IN vaddr: usize - The start of the allocation to free. This should be the address returned from a prior `alloc` call
        ///
        /// Error: Bitmap.BitmapError || VmmError
        ///     VmmError.NotAllocated - This address hasn't been allocated yet
        ///     Bitmap.BitmapError.OutOfBounds - The address is out of the manager's bounds
        ///
        pub fn free(self: *Self, vaddr: usize) (bitmap.BitmapError || VmmError)!void {
            const entry = (vaddr - self.start) / BLOCK_SIZE;
            if (try self.bmp.isSet(entry)) {
                // There will be an allocation associated with this virtual address
                const allocation = self.allocations.get(vaddr).?;
                const physical = allocation.physical;
                defer physical.deinit();
                const num_physical_allocations = physical.items.len;
                for (physical.items) |block, i| {
                    // Clear the address space entry and free the physical memory
                    try self.bmp.clearEntry(entry + i);
                    pmm.free(block) catch |e| {
                        panic(@errorReturnTrace(), "Failed to free PMM reserved memory at 0x{X}: {}\n", .{ block * BLOCK_SIZE, e });
                    };
                }
                // Unmap the entire range
                const region_start = vaddr;
                const region_end = vaddr + (num_physical_allocations * BLOCK_SIZE);
                self.mapper.unmapFn(region_start, region_end, self.allocator, self.payload) catch |e| {
                    panic(@errorReturnTrace(), "Failed to unmap VMM reserved memory from 0x{X} to 0x{X}: {}\n", .{ region_start, region_end, e });
                };
                // The allocation is freed so remove from the map
                assert(self.allocations.remove(vaddr));
            } else {
                return VmmError.NotAllocated;
            }
        }
    };
}

///
/// Initialise the main system virtual memory manager covering 4GB. Maps in the kernel code and reserved virtual memory
///
/// Arguments:
///     IN mem_profile: *const mem.MemProfile - The system's memory profile. This is used to find the kernel code region and boot modules
///     IN/OUT allocator: Allocator - The allocator to use when needing to allocate memory
///
/// Return: VirtualMemoryManager
///     The virtual memory manager created with all reserved virtual regions allocated
///
/// Error: Allocator.Error
///     error.OutOfMemory - The allocator cannot allocate the memory required
///
pub fn init(mem_profile: *const mem.MemProfile, allocator: Allocator) Allocator.Error!*VirtualMemoryManager(arch.VmmPayload) {
    log.info("Init\n", .{});
    defer log.info("Done\n", .{});

    kernel_vmm = try VirtualMemoryManager(arch.VmmPayload).init(@ptrToInt(&KERNEL_ADDR_OFFSET), 0xFFFFFFFF, allocator, arch.VMM_MAPPER, arch.KERNEL_VMM_PAYLOAD);

    // Map all the reserved virtual addresses.
    for (mem_profile.virtual_reserved) |entry| {
        const virtual = mem.Range{
            .start = std.mem.alignBackward(entry.virtual.start, BLOCK_SIZE),
            .end = std.mem.alignForward(entry.virtual.end, BLOCK_SIZE),
        };
        const physical: ?mem.Range = if (entry.physical) |phys|
            mem.Range{
                .start = std.mem.alignBackward(phys.start, BLOCK_SIZE),
                .end = std.mem.alignForward(phys.end, BLOCK_SIZE),
            }
        else
            null;
        kernel_vmm.set(virtual, physical, .{ .kernel = true, .writable = true, .cachable = true }) catch |e| switch (e) {
            VmmError.AlreadyAllocated => {},
            else => panic(@errorReturnTrace(), "Failed mapping region in VMM {X}: {}\n", .{ entry, e }),
        };
    }

    return &kernel_vmm;
}

test "virtToPhys" {
    const num_entries = 512;
    var vmm = try testInit(num_entries);
    defer testDeinit(&vmm);

    const vstart = vmm.start + BLOCK_SIZE;
    const vend = vstart + BLOCK_SIZE * 3;
    const pstart = BLOCK_SIZE * 20;
    const pend = BLOCK_SIZE * 23;

    // Set the physical and virtual back to front to complicate the mappings a bit
    try vmm.set(.{ .start = vstart, .end = vstart + BLOCK_SIZE }, mem.Range{ .start = pstart + BLOCK_SIZE * 2, .end = pend }, .{ .kernel = true, .writable = true, .cachable = true });
    try vmm.set(.{ .start = vstart + BLOCK_SIZE, .end = vend }, mem.Range{ .start = pstart, .end = pstart + BLOCK_SIZE * 2 }, .{ .kernel = true, .writable = true, .cachable = true });

    try std.testing.expectEqual(pstart + BLOCK_SIZE * 2, try vmm.virtToPhys(vstart));
    try std.testing.expectEqual(pstart + BLOCK_SIZE * 2 + 29, (try vmm.virtToPhys(vstart + 29)));
    try std.testing.expectEqual(pstart + 29, (try vmm.virtToPhys(vstart + BLOCK_SIZE + 29)));

    try std.testing.expectError(VmmError.NotAllocated, vmm.virtToPhys(vstart - 1));
    try std.testing.expectError(VmmError.NotAllocated, vmm.virtToPhys(vend));
    try std.testing.expectError(VmmError.NotAllocated, vmm.virtToPhys(vend + 1));
}

test "physToVirt" {
    const num_entries = 512;
    var vmm = try testInit(num_entries);
    defer testDeinit(&vmm);

    const vstart = vmm.start + BLOCK_SIZE;
    const vend = vstart + BLOCK_SIZE * 3;
    const pstart = BLOCK_SIZE * 20;
    const pend = BLOCK_SIZE * 23;

    // Set the physical and virtual back to front to complicate the mappings a bit
    try vmm.set(.{ .start = vstart, .end = vstart + BLOCK_SIZE }, mem.Range{ .start = pstart + BLOCK_SIZE * 2, .end = pend }, .{ .kernel = true, .writable = true, .cachable = true });
    try vmm.set(.{ .start = vstart + BLOCK_SIZE, .end = vend }, mem.Range{ .start = pstart, .end = pstart + BLOCK_SIZE * 2 }, .{ .kernel = true, .writable = true, .cachable = true });

    try std.testing.expectEqual(vstart, try vmm.physToVirt(pstart + BLOCK_SIZE * 2));
    try std.testing.expectEqual(vstart + 29, (try vmm.physToVirt(pstart + BLOCK_SIZE * 2 + 29)));
    try std.testing.expectEqual(vstart + BLOCK_SIZE + 29, (try vmm.physToVirt(pstart + 29)));

    try std.testing.expectError(VmmError.NotAllocated, vmm.physToVirt(pstart - 1));
    try std.testing.expectError(VmmError.NotAllocated, vmm.physToVirt(pend));
    try std.testing.expectError(VmmError.NotAllocated, vmm.physToVirt(pend + 1));
}

test "alloc and free" {
    const num_entries = 512;
    var vmm = try testInit(num_entries);
    defer testDeinit(&vmm);
    var allocations = test_allocations.?;
    var virtual_allocations = std.ArrayList(usize).init(std.testing.allocator);
    defer virtual_allocations.deinit();

    var entry: u32 = 0;
    while (entry < num_entries) {
        // Test allocating various numbers of blocks all at once
        // Rather than using a random number generator, just set the number of blocks to allocate based on how many entries have been done so far
        var num_to_alloc: u32 = if (entry > 400) @as(u32, 8) else if (entry > 320) @as(u32, 14) else if (entry > 270) @as(u32, 9) else if (entry > 150) @as(u32, 26) else @as(u32, 1);
        const result = try vmm.alloc(num_to_alloc, null, .{ .kernel = true, .writable = true, .cachable = true });

        var should_be_set = true;
        if (entry + num_to_alloc > num_entries) {
            // If the number to allocate exceeded the number of entries, then allocation should have failed
            try std.testing.expectEqual(@as(?usize, null), result);
            should_be_set = false;
        } else {
            // Else it should have succeeded and allocated the correct address
            try std.testing.expectEqual(@as(?usize, vmm.start + entry * BLOCK_SIZE), result);
            try virtual_allocations.append(result.?);
        }

        // Make sure that the entries are set or not depending on the allocation success
        var vaddr = vmm.start + entry * BLOCK_SIZE;
        while (vaddr < (entry + num_to_alloc) * BLOCK_SIZE) : (vaddr += BLOCK_SIZE) {
            if (should_be_set) {
                // Allocation succeeded so this address should be set
                try std.testing.expect(try vmm.isSet(vaddr));
                // The test mapper should have received this address
                try std.testing.expect(try allocations.isSet(vaddr / BLOCK_SIZE));
            } else {
                // Allocation failed as there weren't enough free entries
                if (vaddr >= num_entries * BLOCK_SIZE) {
                    // If this address is beyond the VMM's end address, it should be out of bounds
                    try std.testing.expectError(bitmap.BitmapError.OutOfBounds, vmm.isSet(vaddr));
                    try std.testing.expectError(bitmap.BitmapError.OutOfBounds, allocations.isSet(vaddr / BLOCK_SIZE));
                } else {
                    // Else it should not be set
                    try std.testing.expect(!(try vmm.isSet(vaddr)));
                    // The test mapper should not have received this address
                    try std.testing.expect(!(try allocations.isSet(vaddr / BLOCK_SIZE)));
                }
            }
        }
        entry += num_to_alloc;

        // All later entries should not be set
        var later_entry = entry;
        while (later_entry < num_entries) : (later_entry += 1) {
            try std.testing.expect(!(try vmm.isSet(vmm.start + later_entry * BLOCK_SIZE)));
            try std.testing.expect(!(try pmm.isSet(later_entry * BLOCK_SIZE)));
        }
    }

    // Try freeing all allocations
    for (virtual_allocations.items) |alloc| {
        const alloc_group = vmm.allocations.get(alloc);
        try std.testing.expect(alloc_group != null);
        const physical = alloc_group.?.physical;
        // We need to create a copy of the physical allocations since the free call deinits them
        var physical_copy = std.ArrayList(usize).init(std.testing.allocator);
        defer physical_copy.deinit();
        // Make sure they are all reserved in the PMM
        for (physical.items) |phys| {
            try std.testing.expect(try pmm.isSet(phys));
            try physical_copy.append(phys);
        }
        vmm.free(alloc) catch unreachable;
        // This virtual allocation should no longer be in the hashmap
        try std.testing.expectEqual(vmm.allocations.get(alloc), null);
        try std.testing.expect(!try vmm.isSet(alloc));
        // And all its physical blocks should now be free
        for (physical_copy.items) |phys| {
            try std.testing.expect(!try pmm.isSet(phys));
        }
    }
}

test "alloc at a specific address" {
    const num_entries = 100;
    var vmm = try testInit(num_entries);
    defer testDeinit(&vmm);

    const attrs = Attributes{ .writable = true, .cachable = true, .kernel = true };
    // Try allocating at the start
    try std.testing.expectEqual(vmm.alloc(10, vmm.start, attrs), vmm.start);
    // Try that again
    try std.testing.expectEqual(vmm.alloc(5, vmm.start, attrs), null);
    const middle = vmm.start + (vmm.end - vmm.start) / 2;
    // Try allocating at the middle
    try std.testing.expectEqual(vmm.alloc(num_entries / 2, middle, attrs), middle);
    // Allocating after the start and colliding with the middle should be impossible
    try std.testing.expectEqual(vmm.alloc(num_entries / 2, vmm.start + 10 * BLOCK_SIZE, attrs), null);
    // Allocating within the last half should be impossible
    try std.testing.expectEqual(vmm.alloc(num_entries / 4, middle + BLOCK_SIZE, attrs), null);
    // It should still be possible to allocate between the start and middle
    try std.testing.expectEqual(vmm.alloc(num_entries / 2 - 10, vmm.start + 10 * BLOCK_SIZE, attrs), vmm.start + 10 * BLOCK_SIZE);
    // It should now be full
    try std.testing.expectEqual(vmm.bmp.num_free_entries, 0);

    // Allocating at the end and before the start should fail
    try std.testing.expectEqual(vmm.alloc(1, vmm.end, attrs), null);
    try std.testing.expectEqual(vmm.alloc(1, vmm.start - BLOCK_SIZE, attrs), null);
}

test "set" {
    const num_entries = 512;
    var vmm = try testInit(num_entries);
    defer testDeinit(&vmm);

    const vstart = vmm.start + BLOCK_SIZE * 37;
    const vend = vmm.start + BLOCK_SIZE * 46;
    const pstart = BLOCK_SIZE * 37 + 123;
    const pend = BLOCK_SIZE * 46 + 123;
    const attrs = Attributes{ .kernel = true, .writable = true, .cachable = true };
    try vmm.set(.{ .start = vstart, .end = vend }, mem.Range{ .start = pstart, .end = pend }, attrs);
    // Make sure it put the correct address in the map
    try std.testing.expect(vmm.allocations.get(vstart) != null);

    var allocations = test_allocations.?;
    // The entries before the virtual start shouldn't be set
    var vaddr = vmm.start;
    while (vaddr < vstart) : (vaddr += BLOCK_SIZE) {
        try std.testing.expect(!(try allocations.isSet((vaddr - vmm.start) / BLOCK_SIZE)));
    }
    // The entries up until the virtual end should be set
    while (vaddr < vend) : (vaddr += BLOCK_SIZE) {
        try std.testing.expect(try allocations.isSet((vaddr - vmm.start) / BLOCK_SIZE));
    }
    // The entries after the virtual end should not be set
    while (vaddr < vmm.end) : (vaddr += BLOCK_SIZE) {
        try std.testing.expect(!(try allocations.isSet((vaddr - vmm.start) / BLOCK_SIZE)));
    }
}

test "copy" {
    const num_entries = 512;
    var vmm = try testInit(num_entries);
    defer testDeinit(&vmm);

    const attrs = .{ .kernel = true, .cachable = true, .writable = true };
    _ = (try vmm.alloc(24, null, attrs)).?;

    var mirrored = try vmm.copy();
    defer mirrored.deinit();
    try std.testing.expectEqual(vmm.bmp.num_free_entries, mirrored.bmp.num_free_entries);
    try std.testing.expectEqual(vmm.start, mirrored.start);
    try std.testing.expectEqual(vmm.end, mirrored.end);
    try std.testing.expectEqual(vmm.allocations.count(), mirrored.allocations.count());
    var it = vmm.allocations.iterator();
    while (it.next()) |next| {
        for (mirrored.allocations.get(next.key_ptr.*).?.physical.items) |block, i| {
            try std.testing.expectEqual(block, vmm.allocations.get(next.key_ptr.*).?.physical.items[i]);
        }
    }
    try std.testing.expectEqual(vmm.mapper, mirrored.mapper);
    try std.testing.expectEqual(vmm.payload, mirrored.payload);

    // Allocating in the new VMM shouldn't allocate in the mirrored one
    const alloc1 = (try mirrored.alloc(3, null, attrs)).?;
    try std.testing.expectEqual(vmm.allocations.count() + 1, mirrored.allocations.count());
    try std.testing.expectEqual(vmm.bmp.num_free_entries - 3, mirrored.bmp.num_free_entries);
    try std.testing.expectError(VmmError.NotAllocated, vmm.virtToPhys(alloc1));

    // And vice-versa
    _ = (try vmm.alloc(3, null, attrs)).?;
    const alloc3 = (try vmm.alloc(1, null, attrs)).?;
    const alloc4 = (try vmm.alloc(1, null, attrs)).?;
    try std.testing.expectEqual(vmm.allocations.count() - 2, mirrored.allocations.count());
    try std.testing.expectEqual(vmm.bmp.num_free_entries + 2, mirrored.bmp.num_free_entries);
    try std.testing.expectError(VmmError.NotAllocated, mirrored.virtToPhys(alloc3));
    try std.testing.expectError(VmmError.NotAllocated, mirrored.virtToPhys(alloc4));
}

test "copyData from" {
    var vmm = try testInit(100);
    defer testDeinit(&vmm);
    const alloc1_blocks = 1;
    const alloc = (try vmm.alloc(alloc1_blocks, null, .{ .kernel = true, .writable = true, .cachable = true })) orelse unreachable;
    var vmm2 = try VirtualMemoryManager(arch.VmmPayload).init(vmm.start, vmm.end, std.testing.allocator, test_mapper, arch.KERNEL_VMM_PAYLOAD);
    defer vmm2.deinit();
    var vmm_free_entries = vmm.bmp.num_free_entries;
    var vmm2_free_entries = vmm2.bmp.num_free_entries;

    var buff: [4]u8 = [4]u8{ 10, 11, 12, 13 };
    try vmm2.copyData(&vmm, true, buff[0..buff.len], alloc);

    // Make sure they are the same
    var buff2 = @intToPtr([*]u8, alloc)[0..buff.len];
    try std.testing.expectEqualSlices(u8, buff[0..buff.len], buff2);
    try std.testing.expectEqual(vmm_free_entries, vmm.bmp.num_free_entries);
    // TODO Remove the subtraction by one once we are able to free the temp space in copyData
    try std.testing.expectEqual(vmm2_free_entries - 1, vmm2.bmp.num_free_entries);

    // Test NotAllocated
    try std.testing.expectError(VmmError.NotAllocated, vmm2.copyData(&vmm, true, buff[0..buff.len], alloc + alloc1_blocks * BLOCK_SIZE));
    try std.testing.expectEqual(vmm_free_entries, vmm.bmp.num_free_entries);
    try std.testing.expectEqual(vmm2_free_entries - 1, vmm2.bmp.num_free_entries);

    // Test Bitmap.Error.OutOfBounds
    try std.testing.expectError(bitmap.BitmapError.OutOfBounds, vmm2.copyData(&vmm, true, buff[0..buff.len], vmm.end));
    try std.testing.expectError(bitmap.BitmapError.OutOfBounds, vmm.copyData(&vmm2, true, buff[0..buff.len], vmm2.end));
    try std.testing.expectEqual(vmm_free_entries, vmm.bmp.num_free_entries);
    try std.testing.expectEqual(vmm2_free_entries - 1, vmm2.bmp.num_free_entries);
}

test "copyDaya to" {
    var vmm = try testInit(100);
    defer testDeinit(&vmm);
    const alloc1_blocks = 1;
    const alloc = (try vmm.alloc(alloc1_blocks, null, .{ .kernel = true, .writable = true, .cachable = true })) orelse unreachable;
    var vmm2 = try VirtualMemoryManager(arch.VmmPayload).init(vmm.start, vmm.end, std.testing.allocator, test_mapper, arch.KERNEL_VMM_PAYLOAD);
    defer vmm2.deinit();
    var vmm_free_entries = vmm.bmp.num_free_entries;
    var vmm2_free_entries = vmm2.bmp.num_free_entries;

    var buff: [4]u8 = [4]u8{ 10, 11, 12, 13 };
    var buff2 = @intToPtr([*]u8, alloc)[0..buff.len];
    try vmm2.copyData(&vmm, false, buff[0..], alloc);

    try std.testing.expectEqualSlices(u8, buff[0..buff.len], buff2);
    try std.testing.expectEqual(vmm_free_entries, vmm.bmp.num_free_entries);
    try std.testing.expectEqual(vmm2_free_entries - 1, vmm2.bmp.num_free_entries);
}

var test_allocations: ?*bitmap.Bitmap(false, u64, null) = null;
var test_mapper = Mapper(arch.VmmPayload){ .mapFn = testMap, .unmapFn = testUnmap };

///
/// Initialise a virtual memory manager used for testing
///
/// Arguments:
///     IN num_entries: u32 - The number of entries the VMM should track
///
/// Return: VirtualMemoryManager(u8)
///     The VMM constructed
///
/// Error: Allocator.Error
///     OutOfMemory: The allocator couldn't allocate the structures needed
///
pub fn testInit(num_entries: u32) Allocator.Error!VirtualMemoryManager(arch.VmmPayload) {
    if (test_allocations == null) {
        test_allocations = try std.testing.allocator.create(bitmap.Bitmap(false, u64, null));
        test_allocations.?.* = try bitmap.Bitmap(false, u64, null).init(num_entries, std.testing.allocator);
    } else |allocations| {
        var entry: u32 = 0;
        while (entry < allocations.num_entries) : (entry += 1) {
            allocations.clearEntry(entry) catch unreachable;
        }
    }
    const mem_profile = mem.MemProfile{
        .vaddr_end = undefined,
        .vaddr_start = undefined,
        .physaddr_start = undefined,
        .physaddr_end = undefined,
        .mem_kb = num_entries * BLOCK_SIZE / 1024,
        .fixed_allocator = undefined,
        .virtual_reserved = &[_]mem.Map{},
        .physical_reserved = &[_]mem.Range{},
        .modules = &[_]mem.Module{},
    };
    pmm.init(&mem_profile, std.testing.allocator);
    const test_vaddr_start = @ptrToInt(&(try std.testing.allocator.alloc(u8, num_entries * BLOCK_SIZE))[0]);
    kernel_vmm = try VirtualMemoryManager(arch.VmmPayload).init(test_vaddr_start, test_vaddr_start + num_entries * BLOCK_SIZE, std.testing.allocator, test_mapper, arch.KERNEL_VMM_PAYLOAD);
    return kernel_vmm;
}

pub fn testDeinit(vmm: *VirtualMemoryManager(arch.VmmPayload)) void {
    vmm.deinit();
    const space = @intToPtr([*]u8, vmm.start)[0 .. vmm.end - vmm.start];
    vmm.allocator.free(space);
    if (test_allocations) |allocs| {
        allocs.deinit();
        std.testing.allocator.destroy(allocs);
        test_allocations = null;
    }
    pmm.deinit();
}

///
/// A mapping function used when doing unit tests
///
/// Arguments:
///     IN vstart: usize - The start of the virtual region to map
///     IN vend: usize - The end of the virtual region to map
///     IN pstart: usize - The start of the physical region to map
///     IN pend: usize - The end of the physical region to map
///     IN attrs: Attributes - The attributes to map with
///     IN/OUT allocator: Allocator - The allocator to use. Ignored
///     IN payload: arch.VmmPayload - The payload value. Expected to be arch.KERNEL_VMM_PAYLOAD
///
fn testMap(vstart: usize, vend: usize, pstart: usize, pend: usize, attrs: Attributes, allocator: Allocator, payload: arch.VmmPayload) MapperError!void {
    // Suppress unused var warning
    _ = attrs;
    _ = allocator;
    if (vend - vstart != pend - pstart) return MapperError.AddressMismatch;
    std.testing.expectEqual(arch.KERNEL_VMM_PAYLOAD, payload) catch unreachable;
    var vaddr = vstart;
    var allocations = test_allocations.?;
    while (vaddr < vend) : (vaddr += BLOCK_SIZE) {
        allocations.setEntry((vaddr - kernel_vmm.start) / BLOCK_SIZE) catch unreachable;
    }
}

///
/// An unmapping function used when doing unit tests
///
/// Arguments:
///     IN vstart: usize - The start of the virtual region to unmap
///     IN vend: usize - The end of the virtual region to unmap
///     IN payload: arch.VmmPayload - The payload value. Expected to be arch.KERNEL_VMM_PAYLOAD
///
fn testUnmap(vstart: usize, vend: usize, allocator: Allocator, payload: arch.VmmPayload) MapperError!void {
    // Suppress unused var warning
    _ = allocator;
    std.testing.expectEqual(arch.KERNEL_VMM_PAYLOAD, payload) catch unreachable;
    var vaddr = vstart;
    var allocations = test_allocations.?;
    while (vaddr < vend) : (vaddr += BLOCK_SIZE) {
        if (allocations.isSet((vaddr - kernel_vmm.start) / BLOCK_SIZE) catch unreachable) {
            allocations.clearEntry((vaddr - kernel_vmm.start) / BLOCK_SIZE) catch unreachable;
        } else {
            return MapperError.NotMapped;
        }
    }
}

///
/// Run the runtime tests.
///
/// Arguments:
///     IN comptime Payload: type - The type of the payload passed to the mapper
///     IN vmm: VirtualMemoryManager(Payload) - The virtual memory manager to test
///     IN mem_profile: *const mem.MemProfile - The mem profile with details about all the memory regions that should be reserved
///     IN mb_info: *multiboot.multiboot_info_t - The multiboot info struct that should also be reserved
///
pub fn runtimeTests(comptime Payload: type, vmm: *VirtualMemoryManager(Payload), mem_profile: *const mem.MemProfile) void {
    rt_correctMapping(Payload, vmm, mem_profile);
    rt_copyData(vmm);
}

///
/// Test that the correct mappings have been made in the VMM
///
/// Arguments:
///     IN vmm: VirtualMemoryManager(Payload) - The virtual memory manager to test
///     IN mem_profile: *const mem.MemProfile - The mem profile with details about all the memory regions that should be reserved
///
fn rt_correctMapping(comptime Payload: type, vmm: *VirtualMemoryManager(Payload), mem_profile: *const mem.MemProfile) void {
    const v_start = std.mem.alignBackward(@ptrToInt(mem_profile.vaddr_start), BLOCK_SIZE);
    const v_end = std.mem.alignForward(@ptrToInt(mem_profile.vaddr_end), BLOCK_SIZE);

    var vaddr = vmm.start;
    while (vaddr < vmm.end - BLOCK_SIZE) : (vaddr += BLOCK_SIZE) {
        const set = vmm.isSet(vaddr) catch unreachable;
        var should_be_set = false;
        if (vaddr < v_end and vaddr >= v_start) {
            should_be_set = true;
        } else {
            for (mem_profile.virtual_reserved) |entry| {
                if (vaddr >= std.mem.alignBackward(entry.virtual.start, BLOCK_SIZE) and vaddr < std.mem.alignForward(entry.virtual.end, BLOCK_SIZE)) {
                    if (entry.physical) |phys| {
                        const expected_phys = phys.start + (vaddr - entry.virtual.start);
                        if (vmm.virtToPhys(vaddr) catch unreachable != expected_phys) {
                            panic(@errorReturnTrace(), "virtToPhys didn't return the correct physical address for 0x{X} (0x{X})\n", .{ vaddr, vmm.virtToPhys(vaddr) });
                        }
                        if (vmm.physToVirt(expected_phys) catch unreachable != vaddr) {
                            panic(@errorReturnTrace(), "physToVirt didn't return the correct virtual address for 0x{X} (0x{X})\n", .{ expected_phys, vaddr });
                        }
                    }
                    should_be_set = true;
                    break;
                }
            }
        }
        if (set and !should_be_set) {
            panic(@errorReturnTrace(), "An address was set in the VMM when it shouldn't have been: 0x{x}\n", .{vaddr});
        } else if (!set and should_be_set) {
            panic(@errorReturnTrace(), "An address was not set in the VMM when it should have been: 0x{x}\n", .{vaddr});
        }
    }
    log.info("Tested allocations\n", .{});
}

///
/// Test copying data to and from another VMM
///
/// Arguments:
///     IN vmm: *VirtualMemoryManager() - The active VMM to test
///
fn rt_copyData(vmm: *VirtualMemoryManager(arch.VmmPayload)) void {
    const expected_free_entries = vmm.bmp.num_free_entries - 1;
    // Mirror the VMM
    var vmm2 = vmm.copy() catch |e| {
        panic(@errorReturnTrace(), "Failed to mirror VMM: {}\n", .{e});
    };

    // Allocate within secondary VMM
    const addr = vmm2.alloc(1, null, .{ .kernel = true, .cachable = true, .writable = true }) catch |e| {
        panic(@errorReturnTrace(), "Failed to allocate within the secondary VMM in rt_copyData: {}\n", .{e});
    } orelse panic(@errorReturnTrace(), "Failed to get an allocation within the secondary VMM in rt_copyData\n", .{});
    defer vmm2.free(addr) catch |e| {
        panic(@errorReturnTrace(), "Failed to free the allocation in secondary VMM: {}\n", .{e});
    };

    const expected_free_entries2 = vmm2.bmp.num_free_entries;
    const expected_free_pmm_entries = pmm.blocksFree();
    // Try copying to vmm2
    var buff: [6]u8 = [_]u8{ 4, 5, 9, 123, 90, 67 };
    vmm.copyData(&vmm2, true, buff[0..buff.len], addr) catch |e| {
        panic(@errorReturnTrace(), "Failed to copy data to secondary VMM in rt_copyData: {}\n", .{e});
    };
    // Make sure the function cleaned up
    if (vmm.bmp.num_free_entries != expected_free_entries) {
        panic(@errorReturnTrace(), "Expected {} free entries in VMM, but there were {}\n", .{ expected_free_entries, vmm.bmp.num_free_entries });
    }
    if (vmm2.bmp.num_free_entries != expected_free_entries2) {
        panic(@errorReturnTrace(), "Expected {} free entries in the secondary VMM, but there were {}\n", .{ expected_free_entries2, vmm2.bmp.num_free_entries });
    }
    if (pmm.blocksFree() != expected_free_pmm_entries) {
        panic(@errorReturnTrace(), "Expected {} free entries in PMM, but there were {}\n", .{ expected_free_pmm_entries, pmm.blocksFree() });
    }

    // Make sure that the data at the allocated address is correct
    // Since vmm2 is a mirror of vmm, this address should be mapped by the CPU's MMU
    const dest_buff = @intToPtr([*]u8, addr)[0..buff.len];
    if (!std.mem.eql(u8, buff[0..buff.len], dest_buff)) {
        panic(@errorReturnTrace(), "Data copied to vmm2 doesn't have the expected values\n", .{});
    }

    // Now try copying the same buffer from vmm2
    var buff2 = vmm.allocator.alloc(u8, buff.len) catch |e| {
        panic(@errorReturnTrace(), "Failed to allocate a test buffer in rt_copyData: {}\n", .{e});
    };
    vmm.copyData(&vmm2, false, buff2, addr) catch |e| {
        panic(@errorReturnTrace(), "Failed to copy data from secondary VMM in rt_copyData: {}\n", .{e});
    };
    if (!std.mem.eql(u8, buff[0..buff.len], buff2)) {
        panic(@errorReturnTrace(), "Data copied from vmm2 doesn't have the expected values\n", .{});
    }

    // Make sure that a second copy will succeed
    const addr2 = vmm2.alloc(1, null, .{ .kernel = true, .cachable = true, .writable = true }) catch |e| {
        panic(@errorReturnTrace(), "Failed to allocate within the secondary VMM in rt_copyData: {}\n", .{e});
    } orelse panic(@errorReturnTrace(), "Failed to get an allocation within the secondary VMM in rt_copyData\n", .{});
    defer vmm2.free(addr2) catch |e| {
        panic(@errorReturnTrace(), "Failed to free the allocation in secondary VMM: {}\n", .{e});
    };
    const expected_free_entries3 = vmm2.bmp.num_free_entries;
    const expected_free_pmm_entries3 = pmm.blocksFree();
    // Try copying to vmm2
    var buff3: [6]u8 = [_]u8{ 3, 9, 0, 12, 50, 7 };
    vmm.copyData(&vmm2, true, buff3[0..buff3.len], addr) catch |e| {
        panic(@errorReturnTrace(), "Failed to copy third lot of data to secondary VMM in rt_copyData: {}\n", .{e});
    };
    // Make sure the function cleaned up
    if (vmm.bmp.num_free_entries != expected_free_entries - 2) {
        panic(@errorReturnTrace(), "Expected {} free entries in VMM after third copy, but there were {}\n", .{ expected_free_entries - 2, vmm.bmp.num_free_entries });
    }
    if (vmm2.bmp.num_free_entries != expected_free_entries3) {
        panic(@errorReturnTrace(), "Expected {} free entries in the secondary VMM after third copy, but there were {}\n", .{ expected_free_entries2, vmm2.bmp.num_free_entries });
    }
    if (pmm.blocksFree() != expected_free_pmm_entries3) {
        panic(@errorReturnTrace(), "Expected {} free entries in PMM after third copy, but there were {}\n", .{ expected_free_pmm_entries, pmm.blocksFree() });
    }
    // Make sure that the data at the allocated address is correct
    // Since vmm2 is a mirror of vmm, this address should be mapped by the CPU's MMU
    if (!std.mem.eql(u8, buff3[0..buff3.len], dest_buff)) {
        panic(@errorReturnTrace(), "Third lot of data copied doesn't have the expected values\n", .{});
    }
}
